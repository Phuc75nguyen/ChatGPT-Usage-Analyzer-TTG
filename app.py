import io
import json
import math
import zipfile
from datetime import datetime, timezone
from dateutil import tz
from collections import defaultdict

import pandas as pd
import streamlit as st

# ====== CONFIG ======
st.set_page_config(page_title="AI analysis TTG", layout="wide")
LOCAL_TZ = tz.gettz("Asia/Ho_Chi_Minh")

# ====== UTILS ======
def to_dt(ts):
    """Robust timestamp to datetime (local timezone)."""
    if ts is None or (isinstance(ts, float) and math.isnan(ts)):
        return None
    try:
        # ts is usually in seconds; sometimes string
        ts = float(ts)
        # If someone accidentally passes ms, heuristically downscale
        if ts > 1e12:  # ms
            ts = ts / 1000.0
        return datetime.fromtimestamp(ts, tz=timezone.utc).astimezone(LOCAL_TZ)
    except Exception:
        return None

def safe_get_text(msg):
    """Extract text content from ChatGPT export message blocks."""
    if not msg:
        return ""
    # v1: content could be {"content_type":"text","parts":[...]} (old)
    c = msg.get("content")
    if isinstance(c, dict):
        parts = c.get("parts")
        if isinstance(parts, list):
            return "\n".join([str(p) for p in parts if p is not None])
        # newer: content is {"content_type":"text","text": "..."} or tool calls
        if "text" in c and isinstance(c["text"], str):
            return c["text"]
    # v2: some exports put text directly at msg["content"] (string)
    if isinstance(c, str):
        return c
    return ""

def parse_conversation_json(blob, account_label=None):
    """
    Accepts either a dict loaded from conversations.json or a single conversation dict.
    Returns a flat dataframe with columns:
    account, conversation_id, conversation_title, message_id, role, author, text, create_time
    """
    rows = []
    data = blob

    # Some exports wrap in {"conversations": [...]}
    conversations = None
    if isinstance(data, dict) and "conversations" in data and isinstance(data["conversations"], list):
        conversations = data["conversations"]
    else:
        # Might be a single conversation or a top-level dict with many fields
        if isinstance(data, list):
            conversations = data
        else:
            # single conversation?
            conversations = [data]

    for conv in conversations or []:
        conv_id = conv.get("id") or conv.get("conversation_id")
        title = conv.get("title") or conv.get("conversation_title") or ""

        # Newer export: conv["mapping"] (graph of messages)
        if isinstance(conv.get("mapping"), dict):
            for node_id, node in conv["mapping"].items():
                msg = node.get("message") or {}
                if not msg:
                    continue
                role = (msg.get("author") or {}).get("role")
                author_name = (msg.get("author") or {}).get("name")
                text = safe_get_text(msg)
                ts = msg.get("create_time")
                dt = to_dt(ts)

                if role and text:
                    rows.append({
                        "account": account_label,
                        "conversation_id": conv_id,
                        "conversation_title": title,
                        "message_id": msg.get("id") or node_id,
                        "role": role,
                        "author": author_name,
                        "text": text,
                        "create_time": dt
                    })

        # Newer-ish export style: conv["messages"] = [...]
        elif isinstance(conv.get("messages"), list):
            for m in conv["messages"]:
                role = (m.get("author") or {}).get("role") or m.get("role")
                author_name = (m.get("author") or {}).get("name")
                text = safe_get_text(m)
                ts = m.get("create_time") or m.get("timestamp")  # sometimes "timestamp"
                dt = to_dt(ts)
                rows.append({
                    "account": account_label,
                    "conversation_id": conv_id,
                    "conversation_title": title,
                    "message_id": m.get("id"),
                    "role": role,
                    "author": author_name,
                    "text": text,
                    "create_time": dt
                })
        else:
            # Fallback: try best-effort common fields
            pass

    df = pd.DataFrame(rows)
    if not df.empty:
        df["date"] = df["create_time"].dt.date
        df["month"] = df["create_time"].dt.to_period("M").astype(str)
    return df

def month_range(year: int, month: int):
    start = datetime(year, month, 1, tzinfo=LOCAL_TZ)
    if month == 12:
        end = datetime(year + 1, 1, 1, tzinfo=LOCAL_TZ)
    else:
        end = datetime(year, month + 1, 1, tzinfo=LOCAL_TZ)
    return start, end

def filter_month(df, year, month):
    if df.empty:
        return df
    start, end = month_range(year, month)
    return df[(df["create_time"] >= start) & (df["create_time"] < end)].copy()

# ====== PROMPT QUALITY HEURISTICS ======
def score_prompt_quality(text: str):
    """
    Lightweight heuristic: 0-100.
    Criteria:
      - length (tokens ~ words): too short -> low; concise but structured -> good
      - has context markers (background, constraints, examples, role)
      - clear ask verbs (write, summarize, generate, translate, explain, list, compare‚Ä¶)
      - has evaluation/acceptance criteria (format, length caps, style)
    """
    if not text:
        return 0
    t = text.strip()
    words = len(t.split())
    score = 0

    # length
    if words < 5:
        score += 5
    elif words < 15:
        score += 20
    elif words < 60:
        score += 35
    elif words < 200:
        score += 30
    else:
        score += 25  # very long can be noisy, but still ok

    # context signals
    keywords_context = ["b·ªëi c·∫£nh", "context", "d·ªØ li·ªáu", "input", "tham s·ªë", "gi·∫£ ƒë·ªãnh", "assume", "role:", "gi·∫£ s·ª≠","b·∫°n l√†"]
    if any(k.lower() in t.lower() for k in keywords_context):
        score += 15

    # constraints / format
    keywords_constraints = ["ƒë·ªãnh d·∫°ng", "format", "ti√™u ƒë·ªÅ", "m·ª•c", "bullet", "json", "b·∫£ng", "s·ªë t·ª´", "k√Ω t·ª±", "tone"]
    if any(k.lower() in t.lower() for k in keywords_constraints):
        score += 15

    # clear ask verbs
    verbs = ["vi·∫øt", "t√≥m t·∫Øt", "d·ªãch", "gi·∫£i th√≠ch", "li·ªát k√™", "so s√°nh", "ph√¢n t√≠ch", "ƒë∆∞a ra", "t·∫°o", "generate", "summarize", "translate", "explain", "vi·∫øt code"]
    if any(v.lower() in t.lower() for v in verbs):
        score += 15

    # examples
    if "v√≠ d·ª•" in t.lower() or "example" in t.lower() or "m·∫´u" in t.lower():
        score += 10

    # domain terms hint (often better prompts)
    domain_hints = ["h·ª£p ƒë·ªìng", "b√°o c√°o","ph√¢n t√≠ch","m√£ code", "fix code", "bi√™n b·∫£n", "ƒë·∫•u th·∫ßu", "ng√¢n s√°ch", "marketing", "CRM", "quy tr√¨nh", "SOP"]
    if any(d.lower() in t.lower() for d in domain_hints):
        score += 10

    return int(max(0, min(100, score)))

# ====== TOPIC TAGGING (EDITABLE RULES) ======
DEFAULT_TOPIC_RULES = {
    "Kinh doanh": ["doanh thu", "doanh s·ªë", "P&L", "ch·ªët deal", "CRM", "b√°o c√°o kinh doanh"," nh√† cung c·∫•p"],
    "N·ªôi dung/K·∫ø ho·∫°ch": ["qu·∫£ng c√°o", "chi·∫øn d·ªãch", "meta ads", "google ads", "seo", "content", "n·ªôi dung", "th∆∞∆°ng hi·ªáu", "branding", "vi·∫øt", "vƒÉn b·∫£n"],
    "Tuy·ªÉn d·ª•ng/HR": ["jd", "tuy·ªÉn", "ph·ªèng v·∫•n", "ch·∫ø ƒë·ªô", "l∆∞∆°ng", "onboarding", "OKR", "ƒë√°nh gi√°", "performance", "kpi", "ph√∫c l·ª£i", "benefits", "h·ª£p ƒë·ªìng lao ƒë·ªông", "h·ª£p ƒë·ªìng th·ª≠ vi·ªác", "h·ª£p ƒë·ªìng ch√≠nh th·ª©c", "h·ª£p ƒë·ªìng lao ƒë·ªông th·ªùi v·ª•", "h·ª£p ƒë·ªìng c·ªông t√°c vi√™n", "h·ª£p ƒë·ªìng kho√°n vi·ªác","·ª©ng vi√™n", "candidate", "recruitment", "h·ªì s∆° ·ª©ng vi√™n", "job description", "employee handbook"],
    "Ph√°p l√Ω": ["h·ª£p ƒë·ªìng","nh√† th·∫ßu","ch·ªß ƒë·∫ßu t∆∞","g√≥i th·∫ßu", "b√™n a", "b√™n b" ,"ph·ª• l·ª•c", "ƒëi·ªÅu kho·∫£n", "ph√°p l√Ω", "lu·∫≠t", "tu√¢n th·ªß", "compliance", "ki·ªán","tranh ch·∫•p", "th·ªèa thu·∫≠n", "agreement", "legal","kho·∫£n","ƒëi·ªÅu","ngh·ªã ƒë·ªãnh", "ngh·ªã quy·∫øt", ],
    "T√†i ch√≠nh/K·∫ø to√°n": ["h√≥a ƒë∆°n", "ng√¢n s√°ch", "b√∫t to√°n", "b√°o c√°o t√†i ch√≠nh", "thu·∫ø", "vat","invoice", "budget", "accounting", "financial report", "tax", "ki·ªÉm to√°n", "audit", "financial statement", "balance sheet", "profit and loss", "cash flow"],
    "V·∫≠n h√†nh": ["quy tr√¨nh", "SOP", "quy ƒë·ªãnh", "qu·∫£n tr·ªã", "bi·ªÉu m·∫´u", "qu·∫£n l√Ω x√¢y d·ª±ng"],
    "H·ªçc thu·∫≠t": ["h·ªçc thu·∫≠t","so s√°nh" ,"check l·∫°i","nghi√™n c·ª©u","b√†i b√°o", "t√†i li·ªáu h·ªçc thu·∫≠t", "academic", "research", "paper","x·ª≠ l√Ω", "th·∫ø n√†o","lesson plan", "c√¥ng th·ª©c", "c√°ch t√≠nh"],
    "Tra c·ª©u": ["tr√≠ch","tra c·ª©u","vi·∫øt t·∫Øt","bao nhi√™u","v·∫≠y c√≤n","l√†m sao ƒë·ªÉ " ,"l√† g√¨", "l√† sao", "t·∫°i sao" ,"ƒë·ªçc file","t√¨m ki·∫øm", "lookup", "th√¥ng tin", "data", "c∆° s·ªü d·ªØ li·ªáu", "database", "t√†i li·ªáu", "h∆∞·ªõng d·∫´n", "l√† g√¨?"],
    "D·ªãch thu·∫≠t": ["d·ªãch","dich", "dich sang","chuy·ªÉn ng·ªØ", "Chuy·ªÉn sang song ng·ªØ Anh - Vi·ªát", "translate", "phi√™n d·ªãch", "b·∫£n d·ªãch", "ng√¥n ng·ªØ", "ti·∫øng anh", "ti·∫øng vi·ªát"],
    "CSKH": ["h·ªó tr·ª£", "kh√°ch h√†ng", "ph·∫£n h·ªìi", "ticket", "tr·ª£ gi√∫p", "gi·∫£i ƒë√°p"],
    "X√¢y d·ª±ng": ["thi c√¥ng", "c√¥ng tr√¨nh", "d·ª± √°n", "x√¢y d·ª±ng", "ki·∫øn tr√∫c","b√™ t√¥ng", "k·ªπ s∆∞", "gi√°m s√°t", "thi·∫øt k·∫ø ki·∫øn tr√∫c", "construction", "project management", "architectural design", "engineering"],
    "B√°o c√°o": ["b√°o c√°o", "report", "th·ªëng k√™", "ph√¢n t√≠ch", "dashboard", "bi·ªÉu ƒë·ªì", "chart", "data analysis"],
    "H·ªó tr·ª£ Thi·∫øt k·∫ø": ["thi·∫øt k·∫ø", "ƒë·ªì h·ªça", "sketchup", "autocad", "b·∫£n v·∫Ω", "mockup", "prototype", "edit", "vi·ªÅn m·ªèng", "vi·ªÅn d√†y", "m√†u s·∫Øc", "font ch·ªØ", "logo", "branding", "design", "graphic design", "sketchup", "autocad", "photoshop", "illustrator","mm", "cm", "inch", "pixel", "dpi", "resolution", "vector", "raster", "layout", "composition", "typography"],
    "Email": ["email", "th∆∞ ƒëi·ªán t·ª≠", "g·ª≠i mail", "tr·∫£ l·ªùi email", "h·ªôp th∆∞", "outlook", "gmail"],
    "K·ªπ thu·∫≠t/Ph·∫ßn m·ªÅm/IT": ["api","s∆° ƒë·ªì","gi·∫£ l·∫≠p","giao di·ªán","h·ªçc s√¢u","h·ªçc m√°y","ch·ª©c nƒÉng","window","file","code", "router","ngu·ªìn","excel", "word", "pdf", "sharepoint", "database", "script", "server", "deploy", "bug", "log", "python", "sql","in ·∫•n", "s·ª≠a l·ªói", "s·ª≠a ch·ªØa", "b·∫£o tr√¨", "c√†i ƒë·∫∑t", "h·ªá th·ªëng", "m·∫°ng", "network","AI HUb", "chatgpt", "openai", "gpt-3.5", "gpt-4", "llm", "machine learning", "deep learning", "tr√≠ tu·ªá nh√¢n t·∫°o", "artificial intelligence", "chatbot", "model", "training", "fine-tuning", "prompt engineering", "android", "ios", "app development", "web development", "frontend", "backend", "fullstack", "javascript", "react", "vue", "angular", "nodejs", "express", "django", "flask", "ruby on rails", "k√≠ch th∆∞·ªõc", "file excel"],
}

def tag_topic(text, rules):
    t = text.lower()
    for topic, kws in rules.items():
        if any(kw.lower() in t for kw in kws):
            return topic
    return "Kh√°c"

# ====== OPTIONAL: CLUSTERING (if sklearn available) ======
def try_cluster(df_user_prompts, n_clusters=15):
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        texts = df_user_prompts["text"].fillna("").tolist()
        if len(texts) < n_clusters:
            return None
        vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
        X = vec.fit_transform(texts)
        km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
        labels = km.fit_predict(X)
        return labels
    except Exception:
        return None

# ====== UI ======
st.title("üìä ChatGPT Usage Analyzer for TTG ")

st.sidebar.subheader("T·∫£i d·ªØ li·ªáu")
uploaded_files = st.sidebar.file_uploader(
    "K√©o th·∫£ nhi·ªÅu file conversations.json (m·ªói t√†i kho·∫£n/ph√≤ng ban m·ªôt file)",
    type=["json"], accept_multiple_files=True
)

dept_map_file = st.sidebar.file_uploader(
    "T√πy ch·ªçn: CSV map t√†i kho·∫£n ‚ûú Ph√≤ng ban (c·ªôt: account, department)",
    type=["csv"]
)

col1, col2 = st.sidebar.columns(2)
with col1:
    year = st.number_input("NƒÉm", min_value=2023, max_value=2100, value=datetime.now().year, step=1)
with col2:
    month = st.number_input("Th√°ng", min_value=1, max_value=12, value=datetime.now().month, step=1)

st.sidebar.markdown("---")
st.sidebar.subheader("C√†i ƒë·∫∑t ph√¢n t√≠ch")
use_clustering = st.sidebar.checkbox("B·∫≠t clustering ch·ªß ƒë·ªÅ (n·∫øu c√≥ scikit‚Äëlearn)", value=False)

# Editable rules
st.sidebar.markdown("**Rules ph√¢n lo·∫°i ch·ªß ƒë·ªÅ (c√≥ th·ªÉ ch·ªânh):**")
rules_text = st.sidebar.text_area(
    "D·∫°ng JSON (topic -> list t·ª´ kh√≥a)",
    value=json.dumps(DEFAULT_TOPIC_RULES, ensure_ascii=False, indent=2),
    height=220
)
try:
    topic_rules = json.loads(rules_text)
except Exception:
    st.sidebar.error("Rules JSON kh√¥ng h·ª£p l·ªá. D√πng m·∫∑c ƒë·ªãnh.")
    topic_rules = DEFAULT_TOPIC_RULES

# ====== PARSE & COMBINE ======
if uploaded_files:
    frames = []
    for f in uploaded_files:
        try:
            content = json.load(f)
            account_label = f.name.rsplit(".", 1)[0]  # filename as account label
            df = parse_conversation_json(content, account_label=account_label)
            frames.append(df)
        except Exception as e:
            st.error(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c {f.name}: {e}")

    if frames:
        all_df = pd.concat(frames, ignore_index=True)
        # Filter to selected month
        month_df = filter_month(all_df, int(year), int(month))

        # Load department map if given
        if dept_map_file is not None:
            try:
                map_df = pd.read_csv(dept_map_file)
                if not {"account","department"}.issubset(set(map_df.columns.str.lower())):
                    # try non-lowercase
                    need = {"account","department"}
                    if not need.issubset(set(map_df.columns)):
                        raise ValueError("CSV c·∫ßn 2 c·ªôt: account, department")
                # normalize cols
                map_df.columns = [c.lower() for c in map_df.columns]
                month_df = month_df.merge(map_df[["account","department"]], on="account", how="left")
            except Exception as e:
                st.error(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c map CSV: {e}")
                month_df["department"] = None
        else:
            month_df["department"] = None

        # Basic derived fields
        month_df["is_user"] = month_df["role"].eq("user")
        month_df["is_assistant"] = month_df["role"].eq("assistant")

        # Count per conversation
        conv_stats = month_df.groupby(["account","department","conversation_id","conversation_title"], dropna=False).agg(
            first_time=("create_time","min"),
            last_time=("create_time","max"),
            user_prompts=("is_user","sum"),
            assistant_msgs=("is_assistant","sum"),
            total_msgs=("message_id","count"),
            active_days=("date", lambda s: s.nunique())
        ).reset_index()

        # Topic tagging & prompt quality on user prompts
        user_prompts_df = month_df[month_df["is_user"]].copy()
        user_prompts_df["topic"] = user_prompts_df["text"].apply(lambda t: tag_topic(t, topic_rules))
        user_prompts_df["prompt_quality"] = user_prompts_df["text"].apply(score_prompt_quality)

        # Try clustering
        if use_clustering:
            labels = try_cluster(user_prompts_df)
            if labels is not None:
                user_prompts_df["cluster"] = labels
            else:
                user_prompts_df["cluster"] = None
        else:
            user_prompts_df["cluster"] = None

        # ====== OVERVIEW ======
        st.header("T·ªïng quan")
        total_convs = conv_stats["conversation_id"].nunique()
        total_prompts = int(user_prompts_df.shape[0])
        total_msgs = int(month_df.shape[0])
        dep_count = month_df["department"].nunique() if month_df["department"].notna().any() else 0

        kpi1, kpi2, kpi3, kpi4 = st.columns(4)
        kpi1.metric("S·ªë cu·ªôc h·ªôi tho·∫°i", f"{total_convs:,}")
        kpi2.metric("T·ªïng s·ªë prompts (user)", f"{total_prompts:,}")
        kpi3.metric("T·ªïng s·ªë message", f"{total_msgs:,}")
        kpi4.metric("S·ªë ph√≤ng ban", f"{dep_count if dep_count else '-'}")

        # Aggregations
        st.subheader("Theo ph√≤ng ban")
        by_dep = conv_stats.groupby("department", dropna=False).agg(
            conversations=("conversation_id","nunique"),
            user_prompts=("user_prompts","sum"),
            assistant_msgs=("assistant_msgs","sum"),
            total_msgs=("total_msgs","sum")
        ).reset_index().rename(columns={"department":"Ph√≤ng ban"})
        st.dataframe(by_dep)

        st.subheader("Ch·ªß ƒë·ªÅ (d·ª±a tr√™n rules)")
        by_topic = user_prompts_df.groupby(["department","topic"], dropna=False).size().reset_index(name="prompts")
        st.dataframe(by_topic)

        st.subheader("ƒêi·ªÉm ch·∫•t l∆∞·ª£ng prompt")
        qual = user_prompts_df.groupby("department", dropna=False)["prompt_quality"].agg(["count","mean","median","min","max"]).reset_index()
        st.dataframe(qual)

        # ====== DETAILS TABS ======
        tab1, tab2, tab3, tab4 = st.tabs(["Cu·ªôc h·ªôi tho·∫°i", "Prompts (user)", "Assistant tr·∫£ l·ªùi", "D·ªØ li·ªáu g·ªëc"])

        with tab1:
            st.dataframe(conv_stats.sort_values("first_time"))

        with tab2:
            show_cols = ["account","department","conversation_title","create_time","text","topic","prompt_quality","cluster"]
            st.dataframe(user_prompts_df[show_cols].sort_values("create_time"))

        with tab3:
            assistant_df = month_df[month_df["is_assistant"]][["account","department","conversation_title","create_time","text"]]
            st.dataframe(assistant_df.sort_values("create_time"))

        with tab4:
            st.dataframe(month_df.sort_values("create_time"))

        # ====== EXPORTS ======
        st.header("Xu·∫•t b√°o c√°o")
        # Excel
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine="xlsxwriter", datetime_format="yyyy-mm-dd HH:MM") as writer:
            by_dep.to_excel(writer, index=False, sheet_name="Overview_Departments")
            by_topic.to_excel(writer, index=False, sheet_name="Topics")
            qual.to_excel(writer, index=False, sheet_name="Prompt_Quality")
            conv_stats.to_excel(writer, index=False, sheet_name="Conversations")
            user_prompts_df.to_excel(writer, index=False, sheet_name="User_Prompts")
            assistant_df.to_excel(writer, index=False, sheet_name="Assistant_Replies")
            month_df.to_excel(writer, index=False, sheet_name="Raw")

        st.download_button(
            label="‚¨áÔ∏è T·∫£i Excel t·ªïng h·ª£p",
            data=buffer.getvalue(),
            file_name=f"chatgpt_report_{int(year)}_{int(month):02d}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # Optional: export t·ª´ng ph√≤ng ban th√†nh ZIP CSV
        if month_df["department"].notna().any():
            zbuf = io.BytesIO()
            with zipfile.ZipFile(zbuf, "w", zipfile.ZIP_DEFLATED) as zf:
                for dep, dfdep in user_prompts_df.groupby("department"):
                    if pd.isna(dep):
                        depname = "Unknown"
                    else:
                        depname = str(dep)
                    zf.writestr(f"{depname}_user_prompts.csv", dfdep.to_csv(index=False))
            st.download_button(
                "‚¨áÔ∏è T·∫£i ZIP CSV theo ph√≤ng ban",
                data=zbuf.getvalue(),
                file_name=f"user_prompts_by_department_{int(year)}_{int(month):02d}.zip",
                mime="application/zip"
            )

    else:
        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu h·ª£p l·ªá.")
else:
    st.info("H√£y t·∫£i l√™n m·ªôt ho·∫∑c nhi·ªÅu file conversations.json ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

import io
import json
import math
import zipfile
from datetime import datetime, timezone
from dateutil import tz
from collections import defaultdict

import pandas as pd
import streamlit as st

# ====== CONFIG ======
st.set_page_config(page_title="AI analysis TTG", layout="wide")
LOCAL_TZ = tz.gettz("Asia/Ho_Chi_Minh")

# ====== UTILS ======
def to_dt(ts):
    """Robust timestamp to datetime (local timezone)."""
    if ts is None or (isinstance(ts, float) and math.isnan(ts)):
        return None
    try:
        # ts is usually in seconds; sometimes string
        ts = float(ts)
        # If someone accidentally passes ms, heuristically downscale
        if ts > 1e12:  # ms
            ts = ts / 1000.0
        return datetime.fromtimestamp(ts, tz=timezone.utc).astimezone(LOCAL_TZ)
    except Exception:
        return None

def safe_get_text(msg):
    """Extract text content from ChatGPT export message blocks."""
    if not msg:
        return ""
    # v1: content could be {"content_type":"text","parts":[...]} (old)
    c = msg.get("content")
    if isinstance(c, dict):
        parts = c.get("parts")
        if isinstance(parts, list):
            return "\n".join([str(p) for p in parts if p is not None])
        # newer: content is {"content_type":"text","text": "..."} or tool calls
        if "text" in c and isinstance(c["text"], str):
            return c["text"]
    # v2: some exports put text directly at msg["content"] (string)
    if isinstance(c, str):
        return c
    return ""

def parse_conversation_json(blob, account_label=None):
    """
    Accepts either a dict loaded from conversations.json or a single conversation dict.
    Returns a flat dataframe with columns:
    account, conversation_id, conversation_title, message_id, role, author, text, create_time
    """
    rows = []
    data = blob

    # Some exports wrap in {"conversations": [...]}
    conversations = None
    if isinstance(data, dict) and "conversations" in data and isinstance(data["conversations"], list):
        conversations = data["conversations"]
    else:
        # Might be a single conversation or a top-level dict with many fields
        if isinstance(data, list):
            conversations = data
        else:
            # single conversation?
            conversations = [data]

    for conv in conversations or []:
        conv_id = conv.get("id") or conv.get("conversation_id")
        title = conv.get("title") or conv.get("conversation_title") or ""

        # Newer export: conv["mapping"] (graph of messages)
        if isinstance(conv.get("mapping"), dict):
            for node_id, node in conv["mapping"].items():
                msg = node.get("message") or {}
                if not msg:
                    continue
                role = (msg.get("author") or {}).get("role")
                author_name = (msg.get("author") or {}).get("name")
                text = safe_get_text(msg)
                ts = msg.get("create_time")
                dt = to_dt(ts)

                if role and text:
                    rows.append({
                        "account": account_label,
                        "conversation_id": conv_id,
                        "conversation_title": title,
                        "message_id": msg.get("id") or node_id,
                        "role": role,
                        "author": author_name,
                        "text": text,
                        "create_time": dt
                    })

        # Newer-ish export style: conv["messages"] = [...]
        elif isinstance(conv.get("messages"), list):
            for m in conv["messages"]:
                role = (m.get("author") or {}).get("role") or m.get("role")
                author_name = (m.get("author") or {}).get("name")
                text = safe_get_text(m)
                ts = m.get("create_time") or m.get("timestamp")  # sometimes "timestamp"
                dt = to_dt(ts)
                rows.append({
                    "account": account_label,
                    "conversation_id": conv_id,
                    "conversation_title": title,
                    "message_id": m.get("id"),
                    "role": role,
                    "author": author_name,
                    "text": text,
                    "create_time": dt
                })
        else:
            # Fallback: try best-effort common fields
            pass

    df = pd.DataFrame(rows)
    if not df.empty:
        df["date"] = df["create_time"].dt.date
        df["month"] = df["create_time"].dt.to_period("M").astype(str)
    return df

def month_range(year: int, month: int):
    start = datetime(year, month, 1, tzinfo=LOCAL_TZ)
    if month == 12:
        end = datetime(year + 1, 1, 1, tzinfo=LOCAL_TZ)
    else:
        end = datetime(year, month + 1, 1, tzinfo=LOCAL_TZ)
    return start, end

def filter_month(df, year, month):
    if df.empty:
        return df
    start, end = month_range(year, month)
    return df[(df["create_time"] >= start) & (df["create_time"] < end)].copy()

# ====== PROMPT QUALITY HEURISTICS ======
def score_prompt_quality(text: str):
    """
    Lightweight heuristic: 0-100.
    Criteria:
      - length (tokens ~ words): too short -> low; concise but structured -> good
      - has context markers (background, constraints, examples, role)
      - clear ask verbs (write, summarize, generate, translate, explain, list, compare‚Ä¶)
      - has evaluation/acceptance criteria (format, length caps, style)
    """
    if not text:
        return 0
    t = text.strip()
    words = len(t.split())
    score = 0

    # length
    if words < 5:
        score += 5
    elif words < 15:
        score += 20
    elif words < 60:
        score += 35
    elif words < 200:
        score += 30
    else:
        score += 25  # very long can be noisy, but still ok

    # context signals
    keywords_context = ["b·ªëi c·∫£nh", "context", "d·ªØ li·ªáu", "input", "tham s·ªë", "gi·∫£ ƒë·ªãnh", "assume", "role:", "gi·∫£ s·ª≠","b·∫°n l√†"]
    if any(k.lower() in t.lower() for k in keywords_context):
        score += 15

    # constraints / format
    keywords_constraints = ["ƒë·ªãnh d·∫°ng", "format", "ti√™u ƒë·ªÅ", "m·ª•c", "bullet", "json", "b·∫£ng", "s·ªë t·ª´", "k√Ω t·ª±", "tone"]
    if any(k.lower() in t.lower() for k in keywords_constraints):
        score += 15

    # clear ask verbs
    verbs = ["vi·∫øt", "t√≥m t·∫Øt", "d·ªãch", "gi·∫£i th√≠ch", "li·ªát k√™", "so s√°nh", "ph√¢n t√≠ch", "ƒë∆∞a ra", "t·∫°o", "generate", "summarize", "translate", "explain", "vi·∫øt code"]
    if any(v.lower() in t.lower() for v in verbs):
        score += 15

    # examples
    if "v√≠ d·ª•" in t.lower() or "example" in t.lower() or "m·∫´u" in t.lower():
        score += 10

    # domain terms hint (often better prompts)
    domain_hints = ["h·ª£p ƒë·ªìng", "b√°o c√°o","ph√¢n t√≠ch","m√£ code", "fix code", "bi√™n b·∫£n", "ƒë·∫•u th·∫ßu", "ng√¢n s√°ch", "marketing", "CRM", "quy tr√¨nh", "SOP"]
    if any(d.lower() in t.lower() for d in domain_hints):
        score += 10

    return int(max(0, min(100, score)))

# ====== TOPIC TAGGING (EDITABLE RULES) ======
DEFAULT_TOPIC_RULES = {
    "Kinh doanh": ["doanh thu", "doanh s·ªë", "P&L", "ch·ªët deal", "CRM", "b√°o c√°o kinh doanh"," nh√† cung c·∫•p"],
    "N·ªôi dung/K·∫ø ho·∫°ch": ["qu·∫£ng c√°o", "chi·∫øn d·ªãch", "meta ads", "google ads", "seo", "content", "n·ªôi dung", "th∆∞∆°ng hi·ªáu", "branding", "vi·∫øt", "vƒÉn b·∫£n"],
    "Tuy·ªÉn d·ª•ng/HR": ["jd", "tuy·ªÉn", "ph·ªèng v·∫•n", "ch·∫ø ƒë·ªô", "l∆∞∆°ng", "onboarding", "OKR", "ƒë√°nh gi√°", "performance", "kpi", "ph√∫c l·ª£i", "benefits", "h·ª£p ƒë·ªìng lao ƒë·ªông", "h·ª£p ƒë·ªìng th·ª≠ vi·ªác", "h·ª£p ƒë·ªìng ch√≠nh th·ª©c", "h·ª£p ƒë·ªìng lao ƒë·ªông th·ªùi v·ª•", "h·ª£p ƒë·ªìng c·ªông t√°c vi√™n", "h·ª£p ƒë·ªìng kho√°n vi·ªác","·ª©ng vi√™n", "candidate", "recruitment", "h·ªì s∆° ·ª©ng vi√™n", "job description", "employee handbook"],
    "Ph√°p l√Ω": ["h·ª£p ƒë·ªìng","nh√† th·∫ßu","ch·ªß ƒë·∫ßu t∆∞","g√≥i th·∫ßu", "b√™n a", "b√™n b" ,"ph·ª• l·ª•c", "ƒëi·ªÅu kho·∫£n", "ph√°p l√Ω", "lu·∫≠t", "tu√¢n th·ªß", "compliance", "ki·ªán","tranh ch·∫•p", "th·ªèa thu·∫≠n", "agreement", "legal","kho·∫£n","ƒëi·ªÅu","ngh·ªã ƒë·ªãnh", "ngh·ªã quy·∫øt", ],
    "T√†i ch√≠nh/K·∫ø to√°n": ["h√≥a ƒë∆°n", "ng√¢n s√°ch", "b√∫t to√°n", "b√°o c√°o t√†i ch√≠nh", "thu·∫ø", "vat","invoice", "budget", "accounting", "financial report", "tax", "ki·ªÉm to√°n", "audit", "financial statement", "balance sheet", "profit and loss", "cash flow"],
    "V·∫≠n h√†nh": ["quy tr√¨nh", "SOP", "quy ƒë·ªãnh", "qu·∫£n tr·ªã", "bi·ªÉu m·∫´u", "qu·∫£n l√Ω x√¢y d·ª±ng"],
    "H·ªçc thu·∫≠t": ["h·ªçc thu·∫≠t","so s√°nh" ,"check l·∫°i","nghi√™n c·ª©u","b√†i b√°o", "t√†i li·ªáu h·ªçc thu·∫≠t", "academic", "research", "paper","x·ª≠ l√Ω", "th·∫ø n√†o","lesson plan", "c√¥ng th·ª©c", "c√°ch t√≠nh"],
    "Tra c·ª©u": ["tr√≠ch","tra c·ª©u","vi·∫øt t·∫Øt","bao nhi√™u","v·∫≠y c√≤n","l√†m sao ƒë·ªÉ " ,"l√† g√¨", "l√† sao", "t·∫°i sao" ,"ƒë·ªçc file","t√¨m ki·∫øm", "lookup", "th√¥ng tin", "data", "c∆° s·ªü d·ªØ li·ªáu", "database", "t√†i li·ªáu", "h∆∞·ªõng d·∫´n", "l√† g√¨?"],
    "D·ªãch thu·∫≠t": ["d·ªãch","dich", "dich sang","chuy·ªÉn ng·ªØ", "Chuy·ªÉn sang song ng·ªØ Anh - Vi·ªát", "translate", "phi√™n d·ªãch", "b·∫£n d·ªãch", "ng√¥n ng·ªØ", "ti·∫øng anh", "ti·∫øng vi·ªát"],
    "CSKH": ["h·ªó tr·ª£", "kh√°ch h√†ng", "ph·∫£n h·ªìi", "ticket", "tr·ª£ gi√∫p", "gi·∫£i ƒë√°p"],
    "X√¢y d·ª±ng": ["thi c√¥ng", "c√¥ng tr√¨nh", "d·ª± √°n", "x√¢y d·ª±ng", "ki·∫øn tr√∫c","b√™ t√¥ng", "k·ªπ s∆∞", "gi√°m s√°t", "thi·∫øt k·∫ø ki·∫øn tr√∫c", "construction", "project management", "architectural design", "engineering"],
    "B√°o c√°o": ["b√°o c√°o", "report", "th·ªëng k√™", "ph√¢n t√≠ch", "dashboard", "bi·ªÉu ƒë·ªì", "chart", "data analysis"],
    "H·ªó tr·ª£ Thi·∫øt k·∫ø": ["thi·∫øt k·∫ø", "ƒë·ªì h·ªça", "sketchup", "autocad", "b·∫£n v·∫Ω","mockup", "prototype", "edit", "vi·ªÅn m·ªèng", "vi·ªÅn d√†y", "m√†u s·∫Øc", "font ch·ªØ", "logo", "branding", "design", "graphic design", "sketchup", "autocad", "photoshop", "illustrator","mm", "cm", "inch", "pixel", "dpi", "resolution", "vector", "raster", "layout", "composition", "typography"],
    "Email": ["email", "th∆∞ ƒëi·ªán t·ª≠", "g·ª≠i mail", "tr·∫£ l·ªùi email", "h·ªôp th∆∞", "outlook", "gmail"],
    "QLHC" : ["mua", "ch·ªó mua", "t√¨m mua", "ncc", "thi·∫øt b·ªã", "v·∫≠t t∆∞", "vƒÉn ph√≤ng ph·∫©m", "ƒë·∫∑t mua", "ƒë·∫∑t h√†ng", "nh√† cung c·∫•p", "vendor", "procurement", "supply", "supply chain", "NCC"],
    "K·ªπ thu·∫≠t/Ph·∫ßn m·ªÅm/IT": ["api","s∆° ƒë·ªì","gi·∫£ l·∫≠p","giao di·ªán","h·ªçc s√¢u","h·ªçc m√°y","ch·ª©c nƒÉng","window","file","code", "router","ngu·ªìn","excel", "word", "pdf", "sharepoint", "database", "script", "server", "deploy", "bug", "log", "python", "sql","in ·∫•n", "s·ª≠a l·ªói", "s·ª≠a ch·ªØa", "b·∫£o tr√¨", "c√†i ƒë·∫∑t", "h·ªá th·ªëng", "m·∫°ng", "network","AI HUb", "chatgpt", "openai", "gpt-3.5", "gpt-4", "llm", "machine learning", "deep learning", "tr√≠ tu·ªá nh√¢n t·∫°o", "artificial intelligence", "chatbot", "model", "training", "fine-tuning", "prompt engineering", "android", "ios", "app development", "web development", "frontend", "backend", "fullstack", "javascript", "react", "vue", "angular", "nodejs", "express", "django", "flask", "ruby on rails", "k√≠ch th∆∞·ªõc", "file excel"],
}

def tag_topic(text, rules):
    t = text.lower()
    for topic, kws in rules.items():
        if any(kw.lower() in t for kw in kws):
            return topic
    return "Kh√°c"

# ====== PURPOSE TAGGING (additional heuristics for report template) ======
# The template requires a breakdown of "M·ª•c ƒë√≠ch s·ª≠ d·ª•ng" (purpose of use) such as
# Tra c·ª©u, t√≥m t·∫Øt, vi·∫øt mail, h·ªçc t·∫≠p,‚Ä¶  Because the raw data does not
# explicitly encode purpose, we define a simple heuristic mapping from
# keywords to broad purpose categories.  You can extend or adjust this
# dictionary to better fit your data.  Each entry maps a purpose name to a
# list of keywords; if any keyword appears in the prompt text, that purpose
# will be assigned.  If no keywords match, the prompt is tagged as "Kh√°c".
PURPOSE_RULES = {
    "Tra c·ª©u": [
        "tra c·ª©u", "tra cuu", "lookup", "search", "t√¨m", "t√¨m ki·∫øm", "l√† g√¨", "bao nhi√™u", "t·∫°i sao", "how", "what",
    ],
    "T√≥m t·∫Øt": [
        "t√≥m t·∫Øt", "t√≥m tat", "summary", "summarize", "t·ªïng k·∫øt", "ph√¢n t√°ch", "ƒë√∫c k·∫øt", "t√≥m l∆∞·ª£c",
    ],
    "Vi·∫øt mail": [
        "email", "mail", "th∆∞", "g·ª≠i mail", "vi·∫øt mail", "tr·∫£ l·ªùi email", "reply email", "forward",
    ],
    "H·ªçc t·∫≠p": [
        "h·ªçc", "h·ªçc t·∫≠p", "learn", "study", "b√†i h·ªçc", "lesson", "gi·∫£ng", "nghi√™n c·ª©u",
    ],
    "D·ªãch thu·∫≠t": [
        "d·ªãch", "dich", "translate", "d·ªãch sang", "chuy·ªÉn ng·ªØ", "phi√™n d·ªãch",
    ],
    "Vi·∫øt code": [
        "code", "python", "java", "c++", "javascript", "program", "script", "sql", "l·∫≠p tr√¨nh",
    ],
    "B√°o c√°o": [
        "b√°o c√°o", "report", "th·ªëng k√™", "dashboard", "bi·ªÉu ƒë·ªì", "chart",
    ],
    "K·∫ø ho·∫°ch": [
        "k·∫ø ho·∫°ch", "plan", "planning", "project", "d·ª± √°n", "chi·∫øn d·ªãch",
    ],
}

def tag_purpose(text: str, rules: dict = PURPOSE_RULES) -> str:
    """
    Assign a high‚Äëlevel purpose category to a user prompt based on keyword
    matching.  Returns the first matching purpose name, or "Kh√°c" if none
    match.  This is a simple heuristic and can be tuned by editing
    PURPOSE_RULES above.
    """
    if not text:
        return "Kh√°c"
    t_lower = str(text).lower()
    for purpose, kws in rules.items():
        if any(kw.lower() in t_lower for kw in kws):
            return purpose
    return "Kh√°c"

# ====== KPI RATING ======
def evaluate_kpi(prompts: int, active_days: int, unique_topics: int = 0) -> tuple[str, int]:
    """Evaluate ChatGPT usage compliance and return a rating and score.

    Parameters
    ----------
    prompts : int
        Number of user prompts in the period.
    active_days : int
        Number of distinct days on which ChatGPT was used.
    unique_topics : int, optional
        Number of different topic categories covered in the period.

    Returns
    -------
    tuple[str, int]
        A pair containing the rating name (Xu·∫•t s·∫Øc, T·ªët, Kh√°, Th·∫•p) and
        the corresponding KPI percentage (100, 80, 50, 0).

    The heuristic below approximates the company guidelines:
      * Xu·∫•t s·∫Øc (100%): prompts ‚â• 20, active_days ‚â• 10 and unique_topics ‚â• 3
      * T·ªët (80%): prompts ‚â• 10 and active_days ‚â• 5
      * Kh√° (50%): prompts ‚â• 5 or active_days ‚â• 3
      * Th·∫•p (0%): otherwise
    """
    if prompts >= 50 and active_days >= 20 and unique_topics >= 3:
        return "Xu·∫•t s·∫Øc", 100
    if prompts >= 30 and active_days >= 13:
        return "T·ªët", 80
    if prompts >= 20 or active_days >= 10:
        return "Kh√°", 50
    return "Th·∫•p", 0

# ====== OPTIONAL: CLUSTERING (if sklearn available) ======
def try_cluster(df_user_prompts, n_clusters=15):
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        texts = df_user_prompts["text"].fillna("").tolist()
        if len(texts) < n_clusters:
            return None
        vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
        X = vec.fit_transform(texts)
        km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
        labels = km.fit_predict(X)
        return labels
    except Exception:
        return None

# ====== UI ======
st.title("üìä ChatGPT Usage Analyzer for TTG ")

st.sidebar.subheader("T·∫£i d·ªØ li·ªáu")
uploaded_files = st.sidebar.file_uploader(
    "K√©o th·∫£ nhi·ªÅu file conversations.json (m·ªói t√†i kho·∫£n/ph√≤ng ban m·ªôt file)",
    type=["json"], accept_multiple_files=True
)

dept_map_file = st.sidebar.file_uploader(
    "T√πy ch·ªçn: CSV map t√†i kho·∫£n ‚ûú Ph√≤ng ban (c·ªôt: account, department)",
    type=["csv"]
)

col1, col2 = st.sidebar.columns(2)
with col1:
    year = st.number_input("NƒÉm", min_value=2023, max_value=2100, value=datetime.now().year, step=1)
with col2:
    month = st.number_input("Th√°ng", min_value=1, max_value=12, value=datetime.now().month, step=1)

st.sidebar.markdown("---")
st.sidebar.subheader("C√†i ƒë·∫∑t ph√¢n t√≠ch")
use_clustering = st.sidebar.checkbox("B·∫≠t clustering ch·ªß ƒë·ªÅ (n·∫øu c√≥ scikit‚Äëlearn)", value=False)

# Editable rules
st.sidebar.markdown("**Rules ph√¢n lo·∫°i ch·ªß ƒë·ªÅ (c√≥ th·ªÉ ch·ªânh):**")
rules_text = st.sidebar.text_area(
    "D·∫°ng JSON (topic -> list t·ª´ kh√≥a)",
    value=json.dumps(DEFAULT_TOPIC_RULES, ensure_ascii=False, indent=2),
    height=220
)
try:
    topic_rules = json.loads(rules_text)
except Exception:
    st.sidebar.error("Rules JSON kh√¥ng h·ª£p l·ªá. D√πng m·∫∑c ƒë·ªãnh.")
    topic_rules = DEFAULT_TOPIC_RULES

# ====== PARSE & COMBINE ======
if uploaded_files:
    frames = []
    for f in uploaded_files:
        try:
            content = json.load(f)
            # Derive a base name from the filename (without extension).  This name
            # will be used as both the account label and the default department
            # name.  Administrators can rename the JSON file to reflect the
            # actual department before uploading.
            base_name = f.name.rsplit(".", 1)[0]
            df = parse_conversation_json(content, account_label=base_name)
            if not df.empty:
                df["department"] = base_name
            frames.append(df)
        except Exception as e:
            st.error(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c {f.name}: {e}")

    if frames:
        all_df = pd.concat(frames, ignore_index=True)
        # Filter to selected month
        month_df = filter_month(all_df, int(year), int(month))

        # Load department map if given.  The CSV must contain two columns: account and
        # department.  If provided, the department values from the CSV will
        # override the automatically derived department names (taken from the
        # filename).  Rows without a mapping will retain their original
        # department value.  If the CSV cannot be read, we log an error and
        # leave the existing department values untouched.
        if dept_map_file is not None:
            try:
                map_df = pd.read_csv(dept_map_file)
                # Normalize column names to lowercase for matching
                map_df.columns = [c.lower() for c in map_df.columns]
                if not {"account", "department"}.issubset(set(map_df.columns)):
                    raise ValueError("CSV c·∫ßn 2 c·ªôt: account, department")
                # Rename the department column to avoid clobbering the existing one during merge
                map_df = map_df.rename(columns={"department": "department_override"})
                month_df = month_df.merge(map_df[["account", "department_override"]], on="account", how="left")
                # Use the override department where available; otherwise keep the original
                month_df["department"] = month_df["department_override"].combine_first(month_df["department"])
                month_df.drop(columns=["department_override"], inplace=True)
            except Exception as e:
                st.error(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c map CSV: {e}")
                # Keep existing department values
                pass

        # Basic derived fields
        month_df["is_user"] = month_df["role"].eq("user")
        month_df["is_assistant"] = month_df["role"].eq("assistant")

        # Count per conversation
        conv_stats = month_df.groupby(["account","department","conversation_id","conversation_title"], dropna=False).agg(
            first_time=("create_time","min"),
            last_time=("create_time","max"),
            user_prompts=("is_user","sum"),
            assistant_msgs=("is_assistant","sum"),
            total_msgs=("message_id","count"),
            active_days=("date", lambda s: s.nunique())
        ).reset_index()

        # Topic tagging, purpose tagging & prompt quality on user prompts
        user_prompts_df = month_df[month_df["is_user"]].copy()
        user_prompts_df["topic"] = user_prompts_df["text"].apply(lambda t: tag_topic(t, topic_rules))
        user_prompts_df["purpose"] = user_prompts_df["text"].apply(lambda t: tag_purpose(t))
        user_prompts_df["prompt_quality"] = user_prompts_df["text"].apply(score_prompt_quality)
        # compute word count for summary metrics
        user_prompts_df["word_count"] = user_prompts_df["text"].apply(lambda t: len(str(t).split()) if pd.notna(t) else 0)

        # Try clustering
        if use_clustering:
            labels = try_cluster(user_prompts_df)
            if labels is not None:
                user_prompts_df["cluster"] = labels
            else:
                user_prompts_df["cluster"] = None
        else:
            user_prompts_df["cluster"] = None

        # ====== OVERVIEW ======
        st.header("T·ªïng quan")
        total_convs = conv_stats["conversation_id"].nunique()
        total_prompts = int(user_prompts_df.shape[0])
        total_msgs = int(month_df.shape[0])
        dep_count = month_df["department"].nunique() if month_df["department"].notna().any() else 0

        kpi1, kpi2, kpi3, kpi4 = st.columns(4)
        kpi1.metric("S·ªë cu·ªôc h·ªôi tho·∫°i", f"{total_convs:,}")
        kpi2.metric("T·ªïng s·ªë prompts (user)", f"{total_prompts:,}")
        kpi3.metric("T·ªïng s·ªë message", f"{total_msgs:,}")
        kpi4.metric("S·ªë ph√≤ng ban", f"{dep_count if dep_count else '-'}")

        # Aggregations
        st.subheader("Theo ph√≤ng ban")
        by_dep = conv_stats.groupby("department", dropna=False).agg(
            conversations=("conversation_id","nunique"),
            user_prompts=("user_prompts","sum"),
            assistant_msgs=("assistant_msgs","sum"),
            total_msgs=("total_msgs","sum")
        ).reset_index().rename(columns={"department":"Ph√≤ng ban"})
        st.dataframe(by_dep)

        st.subheader("Ch·ªß ƒë·ªÅ (d·ª±a tr√™n rules)")
        by_topic = user_prompts_df.groupby(["department","topic"], dropna=False).size().reset_index(name="prompts")
        st.dataframe(by_topic)

        # The detailed statistics of prompt quality (mean/median/min/max) have been
        # removed per user request.  Prompt quality is now reflected in the KPI
        # compliance section below.

        # ====== SUMMARY DATA FOR EXCEL REPORT ======
        # Prepare a per‚Äëdepartment summary with metrics required by the template
        report_month_str = f"{int(month):02d}-{int(year)}"
        dept_summary_rows = []
        all_departments = sorted([d for d in month_df["department"].dropna().unique()])
        for idx, dep in enumerate(all_departments, start=1):
            convs = conv_stats[conv_stats["department"] == dep]
            prompts_dep = user_prompts_df[user_prompts_df["department"] == dep]
            if convs.empty:
                continue
            conv_count = convs["conversation_id"].nunique()
            avg_msgs = float(convs["total_msgs"].mean()) if not convs.empty else 0
            active_days = month_df[month_df["department"] == dep]["date"].nunique()
            avg_prompt_len = float(prompts_dep["word_count"].mean()) if not prompts_dep.empty else 0
            # Purpose distribution
            purpose_counts = prompts_dep["purpose"].value_counts()
            total_p = purpose_counts.sum()
            purpose_lines = []
            if total_p > 0:
                for p_name, count in purpose_counts.items():
                    pct = count / total_p
                    purpose_lines.append(f"+ {p_name} ({pct:.0%})")
            purpose_str = "\n".join(purpose_lines)
            # Topic distribution
            topic_counts = prompts_dep["topic"].value_counts()
            total_t = topic_counts.sum()
            topic_lines = []
            if total_t > 0:
                for t_name, count in topic_counts.items():
                    pct = count / total_t
                    topic_lines.append(f"+ {t_name} ({pct:.0%})")
            topic_str = "\n".join(topic_lines)
            # Determine KPI evaluation for this department
            prompts_count = len(prompts_dep)
            unique_topics_count = int(prompts_dep["topic"].nunique()) if not prompts_dep.empty else 0
            rating_name, kpi_score = evaluate_kpi(prompts_count, active_days, unique_topics_count)
            dept_summary_rows.append({
                "Ti√™u ƒë·ªÅ": dep,
                "#": idx,
                "Th√°ng": report_month_str,
                "Active Days (s·ªë ng√†y c√≥ s·ª≠ d·ª•ng)": active_days,
                "S·ªë l∆∞·ª£ng h·ªôi tho·∫°i": conv_count,
                "S·ªë l∆∞·ª£ng tin nh·∫Øn trung b√¨nh trong m·ªói h·ªôi tho·∫°i": avg_msgs,
                "ƒê·ªô d√†i trung b√¨nh c√¢u prompt (t·ª´)": avg_prompt_len,
                "M·ª•c ƒë√≠ch s·ª≠ d·ª•ng (Tra c·ª©u, t√≥m t·∫Øt, Vi·∫øt mail, h·ªçc t·∫≠p,...) k√®m %": purpose_str,
                "Ch·ªß ƒë·ªÅ (Tuy·ªÉn d·ª•ng, Thu·∫ø, ...) k√®m %": topic_str,
                "ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng theo A3 (Xu·∫•t s·∫Øc, T·ªët, Kh√°, Th·∫•p)": rating_name,
                "ƒêi·ªÉm KPIs %": kpi_score,
            })
        dept_summary_df = pd.DataFrame(dept_summary_rows)

        # ====== DETAIL VIEWS BY DEPARTMENT ======
        st.header("Chi ti·∫øt theo ph√≤ng ban")
        departments = sorted([d for d in month_df["department"].dropna().unique()])
        if not departments:
            st.info("Kh√¥ng c√≥ ph√≤ng ban n√†o ƒë∆∞·ª£c t√¨m th·∫•y trong d·ªØ li·ªáu.")
        else:
            selected_dep = st.selectbox("Ch·ªçn ph√≤ng ban", options=departments, index=0)
            # Filter dataframes for selected department
            conv_stats_dep = conv_stats[conv_stats["department"] == selected_dep]
            user_prompts_dep = user_prompts_df[user_prompts_df["department"] == selected_dep]
            assistant_dep = month_df[(month_df["department"] == selected_dep) & (month_df["is_assistant"])]
            raw_dep = month_df[month_df["department"] == selected_dep]
            # Tabs for detailed views
            tab_a, tab_b, tab_c, tab_d = st.tabs([
                "Cu·ªôc h·ªôi tho·∫°i", "Prompts (user)", "Assistant tr·∫£ l·ªùi", "D·ªØ li·ªáu g·ªëc"
            ])
            with tab_a:
                st.subheader(f"Danh s√°ch h·ªôi tho·∫°i c·ªßa ph√≤ng ban {selected_dep}")
                st.dataframe(
                    conv_stats_dep.sort_values("first_time").rename(
                        columns={
                            "conversation_id": "ID h·ªôi tho·∫°i",
                            "conversation_title": "Ti√™u ƒë·ªÅ h·ªôi tho·∫°i",
                            "first_time": "B·∫Øt ƒë·∫ßu",
                            "last_time": "K·∫øt th√∫c",
                            "user_prompts": "S·ªë prompt",
                            "assistant_msgs": "S·ªë tr·∫£ l·ªùi",
                            "total_msgs": "T·ªïng tin nh·∫Øn",
                            "active_days": "S·ªë ng√†y ho·∫°t ƒë·ªông",
                        }
                    )
                )
                if not conv_stats_dep.empty:
                    conv_choices = conv_stats_dep[["conversation_id", "conversation_title"]].apply(
                        lambda r: f"{r['conversation_id']} ‚Äì {r['conversation_title']}", axis=1
                    ).tolist()
                    selected_conv_label = st.selectbox(
                        "Ch·ªçn h·ªôi tho·∫°i ƒë·ªÉ xem chi ti·∫øt", options=conv_choices
                    )
                    selected_conv_id = selected_conv_label.split("‚Äì", 1)[0].strip()
                    if st.button("Xem chi ti·∫øt"):
                        show_in_modal = hasattr(st, "modal")
                        container_ctx = st.modal if show_in_modal else st.expander
                        with container_ctx(f"Chi ti·∫øt h·ªôi tho·∫°i: {selected_conv_label}"):
                            conv_msgs = raw_dep[(raw_dep["conversation_id"].astype(str) == selected_conv_id)].sort_values("create_time")
                            if conv_msgs.empty:
                                st.info("Kh√¥ng c√≥ d·ªØ li·ªáu cho h·ªôi tho·∫°i n√†y.")
                            else:
                                for _, row in conv_msgs.iterrows():
                                    ts = row["create_time"].strftime("%Y-%m-%d %H:%M") if pd.notna(row["create_time"]) else ""
                                    if row["role"] == "user":
                                        st.markdown(f"**User ({ts})**: {row['text']}")
                                    else:
                                        st.markdown(f"**Assistant ({ts})**: {row['text']}")
            with tab_b:
                st.subheader(f"Danh s√°ch prompt c·ªßa ph√≤ng ban {selected_dep}")
                show_cols = [
                    "create_time", "conversation_title", "text", "topic", "purpose", "prompt_quality", "word_count", "cluster"
                ]
                st.dataframe(
                    user_prompts_dep[show_cols]
                    .sort_values("create_time")
                    .rename(
                        columns={
                            "create_time": "Th·ªùi gian",
                            "conversation_title": "Ti√™u ƒë·ªÅ h·ªôi tho·∫°i",
                            "text": "N·ªôi dung prompt",
                            "topic": "Ch·ªß ƒë·ªÅ",
                            "purpose": "M·ª•c ƒë√≠ch",
                            "prompt_quality": "ƒêi·ªÉm ch·∫•t l∆∞·ª£ng",
                            "word_count": "S·ªë t·ª´",
                            "cluster": "Nh√≥m",
                        }
                    )
                )
            with tab_c:
                st.subheader(f"C√¢u tr·∫£ l·ªùi c·ªßa Assistant ‚Äì ph√≤ng ban {selected_dep}")
                st.dataframe(
                    assistant_dep[["create_time", "conversation_title", "text"]]
                    .sort_values("create_time")
                    .rename(
                        columns={
                            "create_time": "Th·ªùi gian",
                            "conversation_title": "Ti√™u ƒë·ªÅ h·ªôi tho·∫°i",
                            "text": "N·ªôi dung",
                        }
                    )
                )
            with tab_d:
                st.subheader(f"D·ªØ li·ªáu g·ªëc ‚Äì ph√≤ng ban {selected_dep}")
                st.dataframe(raw_dep.sort_values("create_time"))

        # ====== KPI COMPLIANCE OVERVIEW ======
        st.subheader("Tu√¢n th·ªß quy ƒë·ªãnh v·ªÅ ChatGPT")
        st.markdown(
            """
            **H∆∞·ªõng d·∫´n ch·∫•m ƒëi·ªÉm KPIs**
            
            - **Xu·∫•t s·∫Øc (100%)**: s·ª≠ d·ª•ng th∆∞·ªùng xuy√™n ChatGPT trong c√¥ng vi·ªác h·∫±ng ng√†y, c√≥ nhi·ªÅu n·ªôi dung t∆∞∆°ng t√°c ƒëa d·∫°ng v√† tr·ªçng t√¢m, ƒë√≥ng g√≥p tr·ª±c ti·∫øp v√†o c√¥ng vi·ªác c·ª• th·ªÉ c·∫£ trong chuy√™n m√¥n l·∫´n qu·∫£n tr·ªã v·∫≠n h√†nh, mang l·∫°i hi·ªáu qu·∫£ th·∫•y r√µ cho t·ªï ch·ª©c.
            - **T·ªët (80%)**: s·ª≠ d·ª•ng th∆∞·ªùng xuy√™n ChatGPT trong c√¥ng vi·ªác h·∫±ng ng√†y, c√≥ t·ª´ 5 n·ªôi dung ƒë√≥ng g√≥p v√†o c√¥ng vi·ªác c·ª• th·ªÉ, mang l·∫°i hi·ªáu qu·∫£ th·∫•y r√µ.
            - **Kh√° (50%)**: c√≥ t∆∞∆°ng t√°c s·ª≠ d·ª•ng ChatGPT nh∆∞ng ch∆∞a th∆∞·ªùng xuy√™n ho·∫∑c t∆∞∆°ng t√°c th∆∞·ªùng xuy√™n nh∆∞ng n·ªôi dung ch∆∞a tr·ªçng t√¢m, d·∫´n ƒë·∫øn ·ª©ng d·ª•ng ch∆∞a cao trong c√¥ng vi·ªác.
            - **Th·∫•p (0%)**: √≠t t∆∞∆°ng t√°c tr√™n ·ª©ng d·ª•ng ChatGPT, n·ªôi dung h·ªèi qua loa, ch∆∞a ·ª©ng d·ª•ng ho·∫∑c ·ª©ng d·ª•ng ƒë∆∞·ª£c r·∫•t √≠t trong c√¥ng vi·ªác.
            """,
            unsafe_allow_html=True,
        )
        # Display KPI rating per department
        if not dept_summary_df.empty:
            kpi_table = dept_summary_df[[
                "Ti√™u ƒë·ªÅ",
                "ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng theo A3 (Xu·∫•t s·∫Øc, T·ªët, Kh√°, Th·∫•p)",
                "ƒêi·ªÉm KPIs %",
            ]].rename(columns={
                "Ti√™u ƒë·ªÅ": "Ph√≤ng ban",
                "ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng theo A3 (Xu·∫•t s·∫Øc, T·ªët, Kh√°, Th·∫•p)": "ƒê√°nh gi√°",
                "ƒêi·ªÉm KPIs %": "KPIs (%)",
            })
            st.dataframe(kpi_table)

        # ====== EXPORTS ======
        st.header("Xu·∫•t b√°o c√°o")
        try:
            import xlsxwriter
            buffer = io.BytesIO()
            workbook = xlsxwriter.Workbook(buffer, {"in_memory": True})
            worksheet = workbook.add_worksheet("By Dept")
            # Define formats
            header_fmt = workbook.add_format({
                "bold": True,
                "align": "center",
                "valign": "vcenter",
                "border": 1,
                "text_wrap": True,
            })
            text_fmt = workbook.add_format({
                "align": "left",
                "valign": "top",
                "border": 1,
                "text_wrap": True,
            })
            num_fmt = workbook.add_format({
                "align": "center",
                "valign": "vcenter",
                "border": 1,
                "num_format": "0.00",
            })
            int_fmt = workbook.add_format({
                "align": "center",
                "valign": "vcenter",
                "border": 1,
                "num_format": "0",
            })
            # Header labels
            headers = [
                "Ti√™u ƒë·ªÅ",
                "#",
                "Th√°ng",
                "Active Days (s·ªë ng√†y c√≥ s·ª≠ d·ª•ng)",
                "S·ªë l∆∞·ª£ng h·ªôi tho·∫°i",
                "S·ªë l∆∞·ª£ng tin nh·∫Øn trung b√¨nh trong m·ªói h·ªôi tho·∫°i",
                "ƒê·ªô d√†i trung b√¨nh c√¢u prompt (t·ª´)",
                "M·ª•c ƒë√≠ch s·ª≠ d·ª•ng (Tra c·ª©u, t√≥m t·∫Øt, Vi·∫øt mail, h·ªçc t·∫≠p,...) k√®m %",
                "Ch·ªß ƒë·ªÅ (Tuy·ªÉn d·ª•ng, Thu·∫ø, ...) k√®m %",
                "ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng theo A3 (Xu·∫•t s·∫Øc, T·ªët, Kh√°, Th·∫•p)",
                "ƒêi·ªÉm KPIs %",
            ]
            for col_idx, col_name in enumerate(headers):
                worksheet.write(0, col_idx, col_name, header_fmt)
            # Column widths
            col_widths = [25, 5, 10, 20, 20, 25, 25, 40, 40, 25, 15]
            for i, width in enumerate(col_widths):
                worksheet.set_column(i, i, width)
            # Write data
            for row_idx, row_data in dept_summary_df.iterrows():
                worksheet.write(row_idx + 1, 0, row_data["Ti√™u ƒë·ªÅ"], text_fmt)
                worksheet.write(row_idx + 1, 1, row_data["#"], int_fmt)
                worksheet.write(row_idx + 1, 2, row_data["Th√°ng"], text_fmt)
                worksheet.write(row_idx + 1, 3, row_data["Active Days (s·ªë ng√†y c√≥ s·ª≠ d·ª•ng)"], int_fmt)
                worksheet.write(row_idx + 1, 4, row_data["S·ªë l∆∞·ª£ng h·ªôi tho·∫°i"], int_fmt)
                worksheet.write(row_idx + 1, 5, row_data["S·ªë l∆∞·ª£ng tin nh·∫Øn trung b√¨nh trong m·ªói h·ªôi tho·∫°i"], num_fmt)
                worksheet.write(row_idx + 1, 6, row_data["ƒê·ªô d√†i trung b√¨nh c√¢u prompt (t·ª´)"], num_fmt)
                worksheet.write(row_idx + 1, 7, row_data["M·ª•c ƒë√≠ch s·ª≠ d·ª•ng (Tra c·ª©u, t√≥m t·∫Øt, Vi·∫øt mail, h·ªçc t·∫≠p,...) k√®m %"], text_fmt)
                worksheet.write(row_idx + 1, 8, row_data["Ch·ªß ƒë·ªÅ (Tuy·ªÉn d·ª•ng, Thu·∫ø, ...) k√®m %"], text_fmt)
                worksheet.write(row_idx + 1, 9, row_data["ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng theo A3 (Xu·∫•t s·∫Øc, T·ªët, Kh√°, Th·∫•p)"], text_fmt)
                worksheet.write(row_idx + 1, 10, row_data["ƒêi·ªÉm KPIs %"], int_fmt)
            workbook.close()
            buffer.seek(0)
            st.download_button(
                label="‚¨áÔ∏è T·∫£i Excel b√°o c√°o",
                data=buffer.getvalue(),
                file_name=f"ai_usage_report_{int(year)}_{int(month):02d}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
        except Exception as exc:
            st.error(f"L·ªói khi t·∫°o file Excel: {exc}")

    else:
        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu h·ª£p l·ªá.")
else:
    st.info("H√£y t·∫£i l√™n m·ªôt ho·∫∑c nhi·ªÅu file conversations.json ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
